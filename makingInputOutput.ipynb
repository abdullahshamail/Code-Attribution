{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau, TensorBoard\n",
    "from keras.models import Model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = '/home/hamza/Documents/Hamza-git/data17'\n",
    "\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.cpp' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "# for f in files:\n",
    "#     print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56837"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputvar = {}\n",
    "counter = 0\n",
    "for x in files:\n",
    "    user = x.split('/')[-2]\n",
    "    if user not in outputvar.keys():\n",
    "        outputvar[user] = counter\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(fname):\n",
    "    f = open(fname, \"r\")\n",
    "    y = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    origy = y\n",
    "    while (len(y) < 500):\n",
    "        y += origy\n",
    "        \n",
    "    f = open(fname, \"w\")\n",
    "    f.write(y)\n",
    "    f.close()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    lengths = []\n",
    "    count = 0\n",
    "    for fil in files:\n",
    "        f = open(fil, \"r\")\n",
    "        x = f.read()\n",
    "        f.close()\n",
    "        if(len(x) <= 100):\n",
    "            files.remove(fil)\n",
    "        elif len(x) == 0:\n",
    "            print(fil)\n",
    "            files.remove(fil)\n",
    "        elif(len(x) < 500):\n",
    "            x = padding(fil)\n",
    "            lengths.append(len(x))\n",
    "        else:\n",
    "            lengths.append(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95319\n",
      "500\n",
      "56808\n",
      "56808\n"
     ]
    }
   ],
   "source": [
    "print(max(lengths))\n",
    "print(min(lengths))\n",
    "print(len(lengths))\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/hamza/Documents/Hamza-git/data17/farhit/A1.cpp'\n",
      " '/home/hamza/Documents/Hamza-git/data17/farhit/B1.cpp'\n",
      " '/home/hamza/Documents/Hamza-git/data17/farhit/D0.cpp'\n",
      " '/home/hamza/Documents/Hamza-git/data17/farhit/C0.cpp'\n",
      " '/home/hamza/Documents/Hamza-git/data17/farhit/B2.cpp'\n",
      " '/home/hamza/Documents/Hamza-git/data17/farhit/C1.cpp'\n",
      " '/home/hamza/Documents/Hamza-git/data17/farhit/B0.cpp'\n",
      " '/home/hamza/Documents/Hamza-git/data17/farhit/A0.cpp'\n",
      " '/home/hamza/Documents/Hamza-git/data17/chemthan/practice2.cpp'\n",
      " '/home/hamza/Documents/Hamza-git/data17/chemthan/practice0.cpp']\n",
      "[0 0 0 0 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "inp = np.array(files)\n",
    "names = []\n",
    "for fil in files:\n",
    "    names.append(outputvar[fil.split('/')[-2]])\n",
    "out = np.array(names)\n",
    "print(inp[:10])\n",
    "print(out[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "inp, out, test_size=0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk(path, vocab, count):\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            f = open(os.path.join(r, file))\n",
    "            x = f.read()\n",
    "            for line in x:\n",
    "                for letter in line:\n",
    "                    if letter not in vocab.keys():\n",
    "                        vocab[letter] = count[0]\n",
    "                        count[0] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1468]\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "count = [0]\n",
    "path = '/home/hamza/Documents/Hamza-git/data17/'\n",
    "for r, d, f, in (os.walk(path)):\n",
    "    for i, dir in enumerate(d):\n",
    "        walk(os.path.join(r, dir), vocab, count)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFiles(path):\n",
    "    matrix = []\n",
    "    for p in path:\n",
    "        tmp = np.zeros((50, 10), dtype = int)\n",
    "        f = open(p)\n",
    "        for i in range(0,50):\n",
    "            for j in range(0, 10):\n",
    "                tmp[i][j] = vocab[f.read(1)]\n",
    "        matrix.append(tmp)\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFiles(['/home/hamza/Documents/Hamza-git/data17/0bstacle/main0.cpp']).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(outputvar)\n",
    "input_shape = (50, 10)\n",
    "batch_size = 1000\n",
    "def data_generator(file_paths, author_numbers):\n",
    "    batch_start = 0\n",
    "    batch_end = batch_start + batch_size\n",
    "    n = file_paths.shape[0]\n",
    "    indexes = np.arange(0, n, batch_size)\n",
    "    if n % batch_size != 0:\n",
    "        indexes = indexes[:-1] \n",
    "    while True:\n",
    "        np.random.shuffle(indexes)\n",
    "        for ind in indexes:\n",
    "            batch_start = ind\n",
    "            batch_end = batch_start + batch_size\n",
    "            myFiles = file_paths[batch_start:batch_end]\n",
    "            batch_files = getFiles(myFiles)\n",
    "            batch_authors = author_numbers[batch_start:batch_end]\n",
    "            batch_x = np.zeros((batch_size, 50, 10))\n",
    "            tmp = np.zeros((batch_size, num_classes))\n",
    "            batch_x = np.array( batch_files )\n",
    "            for i, j in enumerate(batch_authors):\n",
    "                tmp[i][j] = 1\n",
    "            batch_y = np.array( tmp )\n",
    "            yield( batch_x, batch_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_fil = Input(shape=(input_shape), name='input_fil')\n",
    "conv = Conv1D(8, kernel_size=3, strides = 2,  activation='relu')(input_fil)\n",
    "maxP = MaxPooling1D()(conv)\n",
    "flat = Flatten()(maxP)\n",
    "output_class = Dense(num_classes, activation='softmax')(flat)\n",
    "\n",
    "model = Model(inputs=input_fil, outputs=output_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_fil (InputLayer)       (None, 50, 10)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 24, 8)             248       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 12, 8)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11609)             1126073   \n",
      "=================================================================\n",
      "Total params: 1,126,321\n",
      "Trainable params: 1,126,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Code-Attribution'\n",
    "\n",
    "if not os.path.exists('./'+model_name):\n",
    "    os.mkdir(model_name)\n",
    "    \n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=adam, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRTensorBoard(TensorBoard):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(LRTensorBoard, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs.update({'lr': K.eval(self.model.optimizer.lr)})\n",
    "        super(LRTensorBoard, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "    \n",
    "checkpoint = ModelCheckpoint(model_name+'/'+model_name+'-{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                             monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "cvslogger = CSVLogger(model_name+'/logs.csv', separator=',', append=True)\n",
    "#reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.000001)\n",
    "tensorboard = LRTensorBoard(log_dir='./'+model_name, histogram_freq=0, write_graph=True, write_grads=1, \n",
    "                            batch_size=batch_size, write_images=True)\n",
    "\n",
    "callbacks = [checkpoint, tensorboard, cvslogger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = data_generator(X_train, Y_train)\n",
    "test_gen = data_generator(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      " 3/11 [=======>......................] - ETA: 23s - loss: 0.0011 - acc: 0.9999"
     ]
    }
   ],
   "source": [
    "hist1 = model.fit_generator(train_gen, epochs=epochs, steps_per_epoch=len(Y_train)//batch_size, \n",
    "                           validation_data=test_gen, validation_steps=len(Y_test)//batch_size, \n",
    "                           callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
